---
layout: page
title: 빅데이터 
subtitle: "S3 &rarr; EC2 서버 - 스파크"
author:
    name: xwMOOC
    url: https://www.facebook.com/groups/tidyverse/
    affiliation: Tidyverse Korea
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
    toc_float: true
    highlight: tango
    code_folding: show
    number_section: true
    self_contained: true
editor_options: 
  chunk_output_type: console
---


``` {r, include=FALSE}
# source("tools/chunk-options.R")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
                    comment="", digits = 3, tidy = FALSE, prompt = FALSE, fig.align = 'center')
```



# S3 백만송 데이터 &rarr; EC2 작업흐름 [^one-million-song-eda] [^music-recommendation] {#million-song-EDA}

[^one-million-song-eda]: [Jingying Zhou, Yibo Zhu, Yimin Zhang, Ziyue Jin, Ziyue Wu (April 27, 2016), "What is mainstream music? Million Songs Dataset Exploration"](https://zac2116.github.io/)
[^music-recommendation]: [databricks, "Predicting Song Listens Using Apache Spark"](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3175648861028866/48824497172554/657465297935335/latest.html)

빅데이터(백만송 데이터)를 분석하기 위해서 빅데이터는 클라우드 AWS S3에 저장하고, 이를 EC2에 설치한 스파크 클러스터를 통해 분석작업을 수행한다.
이를 위해서 다음과 같은 작업흐름을 갖출 수 있다.

1. S3 브라우저와 같은 FTP 프로그램을 이용하여 S3에 데이터를 전송한다.
    - [빅데이터 들어가며 - 기본기: AWS S3 파일 업로드](bigdata-pyspark-prerequisite.html#6_aws_s3_%ED%8C%8C%EC%9D%BC_%EC%97%85%EB%A1%9C%EB%93%9C) 참조
1. EC2 인스턴스에 우선 로컬 스파크 클러스터를 구축한다.
1. 로컬 PC에서 EC2 인스턴스에 RStudio 서버 IDE 로 접속하여 분석작업을 수행한다.

<img src="fig/s3-ec2-rstudio-workflow.png" alt="백만송 S3 업로드" width="67%" />

## 한걸음 더 들어갑니다. {#million-song-EDA-one-step-forward}

한걸음 더 들어가 S3 버킷에 데이터가 저장되어 있는 상태에서 데이터 분석에 필요한 연산작업을 
EC2 스파크 인스턴스를 생성시켜 이를 통해서 추진하는 것이다.
EC2 스파크 인스턴스에 명령을 내리기 위해서 외부 로컬 컴퓨터에서 웹브라우저를 통해 접속하게 된다.

<img src="fig/aws-import-s3-ec2-sparklyr.png" alt="S3-EC2-terminal-toolchain" width="77%" />


# EC2 스파크 &rarr; S3 백만송 데이터 작업흐름 [^one-million-song-eda] [^music-recommendation] {#million-song-EDA}

## 스파크 EC2 설치 {#million-song-spark-install-ec2}

자바는 설치해야 되고, `sudo apt-get install openjdk-8-jdk` 명령어를 통해서 간단히 설치할 수 있다.
그리고 나서 `sparklyr` 팩키지를 설치하고 `spark_install(version = "2.3.2")` 명령어로 스파크를 설치하여 `R`과 연결까지 쉽게 할 수 있다.
RStudio 엔지너어들이 수년동안 노력하면서 버그를 잡아낸 결과 단 한줄의 명령어로 문제를 쉽게 해소할 수 있게 되었다.
`spark_connect()` 함수로 스파크에 연결점을 생성하고 `iris` 데이터프레임을 `copy_to()` 명령어로 던져넣게 되면 
스파크에 `iris` RDD 데이터를 분석할 수 있게 된다. `src_tbls(sc)` 명령어로 분석가능한 RDD가 무엇인지 확인할 수 있고 
`dplyr` 동사로 스파크 클러스터에 올라온 데이터를 분석할 수 있게 되었다.


```{r aws-ec2-spark-install, eval=FALSE}
# 1. 환경설정 -----
# install.packages("devtools")
# devtools::install_github("cloudyr/aws.s3")

library(aws.s3)
library(tidyverse)
library(sparklyr)

# 2. 스파크 설치 -----
spark_install(version = "2.3.2")

# 3. 스파크 연결 -----
sc <- spark_connect(master = "local")

spark_home_dir()

# 4. 데이터 분석작업 -----
iris_tbl <- copy_to(sc, iris)
src_tbls(sc)
[1] "iris"
iris_tbl %>% 
   sample_n(5)
# Source: spark<?> [?? x 5]
  Sepal_Length Sepal_Width Petal_Length Petal_Width Species
*        <dbl>       <dbl>        <dbl>       <dbl> <chr>  
1          5.1         3.5          1.4         0.2 setosa 
2          4.9         3            1.4         0.2 setosa 
3          4.7         3.2          1.3         0.2 setosa 
4          4.6         3.1          1.5         0.2 setosa 
5          5           3.6          1.4         0.2 setosa 
```


```{r aws-ec2-spark-s3-cluster, eval=FALSE}
# 0. 환경설정 -----
library(tidyverse)
library(sparklyr)
library(tictoc)
library(config)
library(aws.s3)
library(aws.signature)

aws <- config::get("aws")

Sys.setenv("AWS_ACCESS_KEY_ID" = aws$AWS_ACCESS_KEY_ID,
           "AWS_SECRET_ACCESS_KEY" = aws$AWS_SECRET_ACCESS_KEY,
           "AWS_DEFAULT_REGION" = "ap-northeast-2")

spark_disconnect(sc)

# 1. 스파크 클러스터 -----
## 1.1. 설치된 SPARK 버젼확인 설정
Sys.getenv("SPARK_HOME")
spark_home_dir()
spark_installed_versions()

## 1.2. 스파크 클러스터 연결
config <- spark_config()
config$sparklyr.defaultPackages <- c("com.databricks:spark-csv_2.10:1.5.0",
                                     "com.amazonaws:aws-java-sdk-pom:1.10.34",
                                     "org.apache.hadoop:hadoop-aws:2.7.3")
config$sparklyr.hadoop.fs.s3a.access.key <- aws$AWS_ACCESS_KEY_ID
config$sparklyr.hadoop.fs.s3a.secret.key <- aws$AWS_SECRET_ACCESS_KEY
# config$sparklyr.hadoop.fs.s3a <- "org.apache.hadoop:fs.s3a.S3AFileSystem"
config$sparklyr.connect.csv.embedded

sc <- spark_connect(master="local", config = config)

## 1.3. 스파크 버젼 확인
spark_version(sc=sc)

# 2. S3 버킷 파일 불러오기 -----
## 2.1. iris.csv 파일 불러오기 -------------------------
# get_bucket("tidyverse-seoul")

iris_rdd <- spark_read_csv(sc, name="iris_data", path="s3a://tidyverse-seoul/iris.csv",
                           infer_schema = TRUE,
                           overwrite = TRUE)
```




